{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forests**\n",
    "Laboratory 5 of the Machine Learning class by Prof. F. Chiariotti at University of Padova during academic year 2024-2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification of Stayed/Churned Customers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Customer Churn table contains information on all 3,758 customers from a Telecommunications company in California in Q2 2022. We want to use this task to predict if a customer will churn (i.e. turn to another company to get a better deal) or not based on three features. It contains three features:\n",
    "- **Tenure** (in Months) Number of months the customer has stayed with the company\n",
    "- **Monthly Charge**: The amount charged to the customer monthly\n",
    "- **Age**: Customer's age\n",
    "- **Customer Status**:  is 0 if the customer has stayed with the company and 1 if the customer has churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # setting the seed\n",
    "\n",
    "def load_dataset(filename):\n",
    "    # function that loads the csv file and splits the dataset between input and labels, while also\n",
    "    # shuffling the data and shifting the label range from [0,1] to [-1,1] (-1 = stayed, +1 = churned)\n",
    "    data_train = pd.read_csv(filename)\n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True) # shuffling\n",
    "    X = data_train.iloc[:, 0:3].values \n",
    "    Y = data_train.iloc[:, 3].values \n",
    "    Y = 2*Y-1 #\n",
    "    return X,Y\n",
    "\n",
    "# loading the dataset\n",
    "X, Y = load_dataset('data/telecom_customer_churn_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to differentiate (classify) between **class \"1\" (churned)** and **class \"-1\" (stayed)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of samples in the train set: 2817\n",
      "# of samples in the test set: 940\n",
      "# of churned users in test: 479\n",
      "# of loyal users in test: 461\n",
      "Mean of the training input data: [-0.  0. -0.]\n",
      "Std of the training input data: [1. 1. 1.]\n",
      "Mean of the test input data: [0.0575483  0.05550169 0.0073833 ]\n",
      "Std of the test input data: [0.98593187 0.97629659 1.00427583]\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into training and test sets\n",
    "m_training = int(0.75*X.shape[0]) # computing the split\n",
    "m_test =  X.shape[0] - m_training\n",
    "X_training =  X[:m_training] # splitting\n",
    "Y_training =  Y[:m_training]\n",
    "X_test =   X[m_training:]\n",
    "Y_test =  Y[m_training:]\n",
    "print(\"# of samples in the train set:\", X_training.shape[0])\n",
    "print(\"# of samples in the test set:\", X_test.shape[0])\n",
    "print(\"# of churned users in test:\", np.sum(Y_test==-1))\n",
    "print(\"# of loyal users in test:\", np.sum(Y_test==1))\n",
    "\n",
    "# standardizing the tests by computing a transformation on the training input set\n",
    "scaler = preprocessing.StandardScaler().fit(X_training)  \n",
    "np.set_printoptions(suppress=True) # setting to zero floating point numbers < min_float_eps\n",
    "# and appling the transformation to both training and test sets\n",
    "X_training =  scaler.transform(X_training)\n",
    "print (\"Mean of the training input data:\", X_training.mean(axis=0))\n",
    "print (\"Std of the training input data:\",X_training.std(axis=0))\n",
    "X_test =  scaler.transform(X_test)\n",
    "print (\"Mean of the test input data:\", X_test.mean(axis=0))\n",
    "print (\"Std of the test input data:\", X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Decision tree**\n",
    "\n",
    "Now **complete** the class *Tree* and all auxiliary functions. <br>\n",
    "\n",
    "The input parameters to pass to the *id3_training* function are:\n",
    "- $X$: the matrix of input features, one row for each sample\n",
    "- $Y$: the vector of labels for the input features matrix X\n",
    "- $max\\_depth$: the maximum depth of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    # implementing a decision tree with two possible training methods:\n",
    "    # id3_training or extra_training\n",
    "\n",
    "    def __init__(self):\n",
    "        # method that initializes a node with the following default values\n",
    "        self.idx = -1 # index of the features used for splitting\n",
    "        self.thresh = 0 # threshold value for splitting\n",
    "        self.leaf = 0 # value of the leaf node\n",
    "        self.left = [] # left and right splits obtained after splitting\n",
    "        self.right = []\n",
    "\n",
    "    def entropy(left, right):\n",
    "        # method that computes the entropy for a split characterized by \n",
    "        # the two sets (left, right) obtained after the split\n",
    "        H = 0\n",
    "        tot_length = len(left) + len(right)\n",
    "        # calculating, for each side, the probability of obtaining that set, and adding to\n",
    "        # the Shanno Entropy with a weight proportional to the size of the set\n",
    "        left_prob = len(np.where(left > 0)[0]) / len(left)\n",
    "        right_prob = len(np.where(right > 0)[0]) / len(right)\n",
    "        if (left_prob > 0):\n",
    "            H -= len(left) * left_prob * np.log2(left_prob) / tot_length\n",
    "        if (left_prob < 1):\n",
    "            H -= len(left) * (1 - left_prob) * np.log2(1 - left_prob) / tot_length\n",
    "        if (right_prob > 0):\n",
    "            H -= len(right) * right_prob * np.log2(right_prob) / tot_length\n",
    "        if (right_prob < 1):\n",
    "            H -= len(right) * (1 - right_prob) * np.log2(1 - right_prob) / tot_length\n",
    "        return H\n",
    "\n",
    "    def classify(self, x):\n",
    "        # method that recursively classifies the input data point x\n",
    "        if (self.leaf == 0): # if the node is not a leaf\n",
    "            if (x[self.idx] > self.thresh): # we compare the feature with the threshold\n",
    "                return self.right.classify(x) # recur on right tree\n",
    "            else:\n",
    "                return self.left.classify(x) # recur on left tree\n",
    "        else:\n",
    "            return self.leaf # if the node is a leaf, then the leaf value is returned\n",
    "\n",
    "    def id3_training(self, X, Y, max_depth):\n",
    "        # method that trains the decision tree based on the ID3 algorithm on the\n",
    "        # training set (X,Y), given an input maximum depth\n",
    "        # if the node is a leaf (i.e. all nodes have the same label), the leaf is set\n",
    "        # to that value and the algorithm is stopped\n",
    "        if (np.max(Y) - np.min(Y) < 1e-3): \n",
    "            self.leaf = np.max(Y) \n",
    "            return\n",
    "        if (max_depth < 1): # if the maximum depth is 0, the node must be a leaf\n",
    "            if (len(np.where(Y > 0)) > len(Y) / 2):\n",
    "                self.leaf = 1\n",
    "            else:\n",
    "                self.leaf = -1\n",
    "            return\n",
    "        best_idx = -1\n",
    "        best_thresh = -1\n",
    "        best_entropy = 1e9\n",
    "        # iterating over the features to find the best split\n",
    "        for idx in range(X.shape[1]): # for each feature\n",
    "            values = X[:, idx]\n",
    "            sorted_ind = np.argsort(values)\n",
    "            values = np.unique(values[sorted_ind])\n",
    "            # all possible thresholds are found as midpoints of uniquely sorted values\n",
    "            thresh = (values[:-1] + values[1:])/2\n",
    "            for j in range(len(values) - 1): # for each split\n",
    "                # splitting into left and right set\n",
    "                left = np.where(X[:, idx] <= thresh[j])[0]\n",
    "                right = np.where(X[:, idx] > thresh[j])[0]\n",
    "                if (len(left) == 0 or len(right) == 0):\n",
    "                    print('error!',best_idx,thresh[j],values)\n",
    "                H = Tree.entropy(Y[left], Y[right]) # calculating the entropy\n",
    "                if (H < best_entropy): # and only the minimum value found is kept/saved\n",
    "                    best_idx = idx\n",
    "                    best_thresh = thresh[j]\n",
    "                    best_entropy = H\n",
    "        if (best_idx == -1): # if no optimal split was found in either directions, all points are identical\n",
    "            self.leaf = np.sign(np.sum(Y))\n",
    "            if (self.leaf == 0):\n",
    "                self.leaf = 1\n",
    "            return\n",
    "        # splitting into left and right set based on the best split\n",
    "        left_samples = np.where(X[:, best_idx] <= best_thresh)[0]\n",
    "        right_samples = np.where(X[:, best_idx] > best_thresh)[0]\n",
    "        self.idx = best_idx\n",
    "        self.thresh = best_thresh\n",
    "        # creating left and right subtrees and recursively training them with the corresponding data\n",
    "        self.left = Tree()\n",
    "        self.right = Tree()\n",
    "        self.left.id3_training(X[left_samples, :], Y[left_samples], max_depth - 1)\n",
    "        self.right.id3_training(X[right_samples, :], Y[right_samples], max_depth - 1)\n",
    "\n",
    "    def extra_training(self, X, Y, max_depth):\n",
    "        # if the node is a leaf (i.e. all nodes have the same label), the leaf is set\n",
    "        # to that value and the algorithm is stopped\n",
    "        if (np.max(Y) - np.min(Y) < 1e-3):\n",
    "            self.leaf = np.max(Y)\n",
    "            return\n",
    "        if (max_depth < 1): # if the maximum depth is 0, the node must be a leaf\n",
    "            if (len(np.where(Y > 0)) > len(Y) / 2):\n",
    "                self.leaf = 1\n",
    "            else:\n",
    "                self.leaf = -1\n",
    "            return\n",
    "        best_idx = -1\n",
    "        best_thresh = -1\n",
    "        best_entropy = 1e9\n",
    "        # iterating over the features to find the best split\n",
    "        for idx in range(X.shape[1]): # for each feature\n",
    "            values = X[:, idx]\n",
    "            sorted_ind = np.argsort(values)\n",
    "            values = np.unique(values[sorted_ind])\n",
    "            if (len(values) > 1):\n",
    "                # selecting a random threshold in that range of values\n",
    "                thresh = rnd.uniform(np.min(values), np.max(values))\n",
    "                # splitting into left and right set based on the random split\n",
    "                left = np.where(X[:, idx] <= thresh)[0]\n",
    "                right = np.where(X[:, idx] > thresh)[0]\n",
    "                H = Tree.entropy(left, right) # computing the entropy\n",
    "                if (H < best_entropy):\n",
    "                    best_idx = idx\n",
    "                    best_thresh = thresh\n",
    "                    best_entropy = H\n",
    "        if (best_idx == -1):  # if no optimal split was found in either directions, all points are identical\n",
    "            self.leaf = np.sign(np.sum(Y))\n",
    "            if (self.leaf == 0):\n",
    "                self.leaf = 1\n",
    "            return\n",
    "        # splitting into left and right set based on the best random split\n",
    "        left_samples = np.where(X[:, best_idx] <= best_thresh)[0]\n",
    "        right_samples = np.where(X[:, best_idx] > best_thresh)[0]\n",
    "        self.idx = best_idx\n",
    "        self.thresh = best_thresh\n",
    "        # creating left and right subtrees and recursively training them with the corresponding data\n",
    "        self.left = Tree()\n",
    "        self.right = Tree()\n",
    "        self.left.extra_training(X[left_samples, :], Y[left_samples], max_depth - 1)\n",
    "        self.right.extra_training(X[right_samples, :], Y[right_samples], max_depth - 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the implementation to learn a model from the training data directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deterministic single tree\n",
      "training loss: 0.19524316648917342\n",
      "test loss: 0.3276595744680835\n"
     ]
    }
   ],
   "source": [
    "single_tree = Tree()\n",
    "single_tree.id3_training(X_training, Y_training, 12) # maximum depth is set to 12\n",
    "\n",
    "print('deterministic single tree')\n",
    "train_loss = 0\n",
    "for i in range(len(Y_training)):\n",
    "    predicted = single_tree.classify(X_training[i, :])\n",
    "    if (Y_training[i] != predicted):\n",
    "        train_loss += 1 / len(Y_training)\n",
    "print('training loss: ' + str(train_loss))\n",
    "\n",
    "test_loss = 0\n",
    "for i in range(len(Y_test)):\n",
    "    predicted = single_tree.classify(X_test[i, :])\n",
    "    if (Y_test[i] != predicted):\n",
    "        test_loss += 1 / len(Y_test)\n",
    "print('test loss: ' + str(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the big difference between training and test loss, we understand that the single tree is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forest:\n",
    "    # implementation of a Random Forest, i.e. a combination of multple decision trees (Tree)\n",
    "    # to improve predicting performance and reduce overfitting\n",
    "\n",
    "    def __init__(self, trees, n_features, n_samples, max_depth):\n",
    "        # method for building the forest\n",
    "        self.forest = [] # a list of decision trees (see Tree)\n",
    "        self.features = n_features # number of features to be exhamined for each tree\n",
    "        self.max_depth = max_depth # maximum depth of each tree\n",
    "        self.samples = n_samples # number of data samples to train each tree on\n",
    "        for i in range(trees):\n",
    "            self.forest.append(Tree())\n",
    "\n",
    "    def classify(self, x):\n",
    "        # method for classifying a single data point x based on the forest\n",
    "        vote = 0\n",
    "        for tree in self.forest:\n",
    "            vote += tree.classify(x)\n",
    "        return np.sign(vote)\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        # method for training the forest\n",
    "        for tree in self.forest: # by training each tree separately\n",
    "            X_train, Y_train = self.bag(X, Y)\n",
    "            tree.id3_training(X_train, Y_train, self.max_depth)\n",
    "        \n",
    "    def bag(self, X, Y):\n",
    "        # method for boothstrapping (i.e. sampling with replacement) and\n",
    "        # feature subsampling\n",
    "        features = X.shape[1]\n",
    "        points = X.shape[0]\n",
    "        bagged = rnd.choices(range(points), k = self.samples)\n",
    "        X_bagged = X[bagged, :]\n",
    "        # randomly selecting which features to keep\n",
    "        selected = rnd.sample(range(features), k = self.features)\n",
    "        for i in range(features): \n",
    "            if i not in selected:\n",
    "                X_bagged[:, i] = 0\n",
    "        return X_bagged, Y[bagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.292553191489361\n"
     ]
    }
   ],
   "source": [
    "# creating a forest of 400 trees, with a maximum depth of 12\n",
    "# since there are not that many features, we keep all 3\n",
    "# we decide to train each tree on all the training point\n",
    "# (bagging still adds some randomness)\n",
    "forest = Forest(400, 3, X_training.shape[0], 12)\n",
    "forest.train(X_training, Y_training)\n",
    "test_loss = 0\n",
    "for i in range(len(Y_test)):\n",
    "    predicted = forest.classify(X_test[i, :])\n",
    "    if (Y_test[i] * predicted <= 0):\n",
    "        test_loss += 1 / len(Y_test)\n",
    "print('Test loss: ' + str(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Extra Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtraTrees:\n",
    "    # implementing thr Extremely Randomized Tree classifier, which introduces more\n",
    "    # bias but also reduces variance\n",
    "\n",
    "    def __init__(self, trees, n_features, n_samples, max_depth):\n",
    "        # method for building the forest\n",
    "        self.forest = [] # a list of decision trees (see Tree)\n",
    "        self.features = n_features # number of features to be exhamined for each tree\n",
    "        self.max_depth = max_depth # maximum depth of each tree\n",
    "        self.samples = n_samples # number of data samples to train each tree on\n",
    "        for i in range(trees):\n",
    "            self.forest.append(Tree())\n",
    "\n",
    "    def classify(self, x):\n",
    "        # method for classifying a single data point x based on the forest\n",
    "        vote = 0\n",
    "        for tree in self.forest:\n",
    "            vote += tree.classify(x)\n",
    "        return np.sign(vote)\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        # method for training the forest\n",
    "        for tree in self.forest: # by training each tree separately\n",
    "            X_train, Y_train = self.bag(X, Y)\n",
    "            tree.extra_training(X_train, Y_train, self.max_depth)\n",
    "        \n",
    "    def bag(self, X, Y):\n",
    "        # method for boothstrapping (i.e. sampling with replacement) and\n",
    "        # feature subsampling\n",
    "        features = X.shape[1]\n",
    "        points = X.shape[0]\n",
    "        bagged = rnd.choices(range(points), k = self.samples)\n",
    "        X_bagged = X[bagged, :]\n",
    "        # randomly selecting which features to keep\n",
    "        selected = rnd.sample(range(features), k = self.features)\n",
    "        for i in range(features):\n",
    "            if i not in selected:\n",
    "                X_bagged[:, i] = 0\n",
    "        return X_bagged, Y[bagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.25744680851063845\n"
     ]
    }
   ],
   "source": [
    "# creating a Extra Tree Classifier of 1000 trees, with a maximum depth of 12\n",
    "# since there are not that many features, we keep all 3\n",
    "# we decide to train each tree on all the training point\n",
    "# (bagging still adds some randomness)\n",
    "\n",
    "extra = ExtraTrees(1000, 3, X_training.shape[0], 12)\n",
    "extra.train(X_training, Y_training)\n",
    "test_loss = 0\n",
    "for i in range(len(Y_test)):\n",
    "    predicted = extra.classify(X_test[i, :])\n",
    "    if (Y_test[i] * predicted <= 0):\n",
    "        test_loss += 1 / len(Y_test)\n",
    "print('Test loss: ' + str(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
